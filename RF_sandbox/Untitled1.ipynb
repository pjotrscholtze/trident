{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "harmful-practice",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dm-acme in ./venv/lib/python3.8/site-packages (0.2.0)\n",
      "Requirement already satisfied: numpy in ./venv/lib/python3.8/site-packages (from dm-acme) (1.19.5)\n",
      "Requirement already satisfied: dm-tree in ./venv/lib/python3.8/site-packages (from dm-acme) (0.1.5)\n",
      "Requirement already satisfied: dm-env in ./venv/lib/python3.8/site-packages (from dm-acme) (1.3)\n",
      "Requirement already satisfied: pillow in ./venv/lib/python3.8/site-packages (from dm-acme) (7.2.0)\n",
      "Requirement already satisfied: absl-py in ./venv/lib/python3.8/site-packages (from dm-acme) (0.11.0)\n",
      "Requirement already satisfied: six>=1.12.0 in ./venv/lib/python3.8/site-packages (from dm-tree->dm-acme) (1.15.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.0 is available.\n",
      "You should consider upgrading via the '/home/pjotr/projects/poc/tutorials/datascience/venv/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n",
      "zsh:1: no matches found: dm-acme[reverb]\n",
      "zsh:1: no matches found: dm-acme[tf]\n",
      "Requirement already satisfied: gym in ./venv/lib/python3.8/site-packages (0.10.11)\n",
      "Requirement already satisfied: numpy>=1.10.4 in ./venv/lib/python3.8/site-packages (from gym) (1.19.5)\n",
      "Requirement already satisfied: pyglet>=1.2.0 in ./venv/lib/python3.8/site-packages (from gym) (1.3.2)\n",
      "Requirement already satisfied: requests>=2.0 in ./venv/lib/python3.8/site-packages (from gym) (2.25.1)\n",
      "Requirement already satisfied: six in ./venv/lib/python3.8/site-packages (from gym) (1.15.0)\n",
      "Requirement already satisfied: scipy in ./venv/lib/python3.8/site-packages (from gym) (1.6.0)\n",
      "Requirement already satisfied: future in ./venv/lib/python3.8/site-packages (from pyglet>=1.2.0->gym) (0.18.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.8/site-packages (from requests>=2.0->gym) (2020.12.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in ./venv/lib/python3.8/site-packages (from requests>=2.0->gym) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in ./venv/lib/python3.8/site-packages (from requests>=2.0->gym) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./venv/lib/python3.8/site-packages (from requests>=2.0->gym) (1.26.2)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.0 is available.\n",
      "You should consider upgrading via the '/home/pjotr/projects/poc/tutorials/datascience/venv/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install dm-acme\n",
    "!pip install dm-acme[reverb]\n",
    "!pip install dm-acme[tf]\n",
    "!pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "opposite-luxury",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment_library = 'gym'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "elementary-elephant",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "\n",
    "from acme import environment_loop\n",
    "from acme import specs\n",
    "from acme import wrappers\n",
    "from acme.agents.tf import d4pg\n",
    "from acme.tf import networks\n",
    "from acme.tf import utils as tf2_utils\n",
    "from acme.utils import loggers\n",
    "import numpy as np\n",
    "import sonnet as snt\n",
    "import gym\n",
    "import time\n",
    "\n",
    "\n",
    "environment = gym.make('MountainCarContinuous-v0')\n",
    "environment = wrappers.GymWrapper(environment)  # To dm_env interface.\n",
    "\n",
    "# Make sure the environment outputs single-precision floats.\n",
    "environment = wrappers.SinglePrecisionWrapper(environment)\n",
    "\n",
    "# Grab the spec of the environment.\n",
    "environment_spec = specs.make_environment_spec(environment)\n",
    "\n",
    "#@title Build agent networks\n",
    "\n",
    "# Get total number of action dimensions from action spec.\n",
    "num_dimensions = np.prod(environment_spec.actions.shape, dtype=int)\n",
    "\n",
    "# Create the shared observation network; here simply a state-less operation.\n",
    "observation_network = tf2_utils.batch_concat\n",
    "\n",
    "# Create the deterministic policy network.\n",
    "policy_network = snt.Sequential([\n",
    "    networks.LayerNormMLP((256, 256, 256), activate_final=True),\n",
    "    networks.NearZeroInitializedLinear(num_dimensions),\n",
    "    networks.TanhToSpec(environment_spec.actions),\n",
    "])\n",
    "\n",
    "# Create the distributional critic network.\n",
    "critic_network = snt.Sequential([\n",
    "    # The multiplexer concatenates the observations/actions.\n",
    "    networks.CriticMultiplexer(),\n",
    "    networks.LayerNormMLP((512, 512, 256), activate_final=True),\n",
    "    networks.DiscreteValuedHead(vmin=-150., vmax=150., num_atoms=51),\n",
    "])\n",
    "\n",
    "# Create a logger for the agent and environment loop.\n",
    "agent_logger = loggers.TerminalLogger(label='agent', time_delta=10.)\n",
    "env_loop_logger = loggers.TerminalLogger(label='env_loop', time_delta=10.)\n",
    "\n",
    "# Create the D4PG agent.\n",
    "agent = d4pg.D4PG(\n",
    "    environment_spec=environment_spec,\n",
    "    policy_network=policy_network,\n",
    "    critic_network=critic_network,\n",
    "    observation_network=observation_network,\n",
    "    sigma=1.0,\n",
    "    logger=agent_logger,\n",
    "    checkpoint=False\n",
    ")\n",
    "\n",
    "# Create an loop connecting this agent to the environment created above.\n",
    "env_loop = environment_loop.EnvironmentLoop(\n",
    "    environment, agent, logger=env_loop_logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "likely-buddy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Agent] Critic Loss = 3.374 | Policy Loss = 0.007 | Steps = 1125 | Walltime = 134.393\n",
      "[Env Loop] Episode Length = 999 | Episode Return = -57.16551971435547 | Episodes = 11 | Steps = 10989 | Steps Per Second = 267.319\n",
      "[Agent] Critic Loss = 3.224 | Policy Loss = 0.004 | Steps = 1459 | Walltime = 144.411\n",
      "[Env Loop] Episode Length = 999 | Episode Return = -51.51047134399414 | Episodes = 14 | Steps = 13986 | Steps Per Second = 258.536\n",
      "[Agent] Critic Loss = 3.058 | Policy Loss = 0.001 | Steps = 1779 | Walltime = 154.437\n",
      "[Env Loop] Episode Length = 999 | Episode Return = -61.6729850769043 | Episodes = 17 | Steps = 16983 | Steps Per Second = 240.101\n",
      "[Agent] Critic Loss = 2.912 | Policy Loss = 0.007 | Steps = 2078 | Walltime = 164.461\n",
      "[Agent] Critic Loss = 2.767 | Policy Loss = 0.000 | Steps = 2371 | Walltime = 174.489\n",
      "[Env Loop] Episode Length = 999 | Episode Return = -61.63372039794922 | Episodes = 20 | Steps = 19980 | Steps Per Second = 232.572\n",
      "[Agent] Critic Loss = 2.650 | Policy Loss = 0.006 | Steps = 2657 | Walltime = 184.491\n",
      "[Env Loop] Episode Length = 999 | Episode Return = -52.324615478515625 | Episodes = 23 | Steps = 22758 | Steps Per Second = 227.334\n",
      "[Agent] Critic Loss = 2.521 | Policy Loss = 0.003 | Steps = 2927 | Walltime = 194.537\n",
      "[Env Loop] Episode Length = 999 | Episode Return = -53.32559585571289 | Episodes = 26 | Steps = 25755 | Steps Per Second = 177.653\n",
      "[Agent] Critic Loss = 2.426 | Policy Loss = 0.006 | Steps = 3150 | Walltime = 204.573\n",
      "[Env Loop] Episode Length = 999 | Episode Return = -53.322357177734375 | Episodes = 28 | Steps = 27753 | Steps Per Second = 192.364\n",
      "[Agent] Critic Loss = 2.316 | Policy Loss = 0.006 | Steps = 3383 | Walltime = 214.576\n",
      "[Env Loop] Episode Length = 999 | Episode Return = -52.742252349853516 | Episodes = 30 | Steps = 29751 | Steps Per Second = 185.824\n",
      "[Agent] Critic Loss = 2.184 | Policy Loss = 0.012 | Steps = 3612 | Walltime = 224.593\n",
      "[Agent] Critic Loss = 2.096 | Policy Loss = 0.019 | Steps = 3856 | Walltime = 234.625\n",
      "[Env Loop] Episode Length = 436 | Episode Return = 75.18161010742188 | Episodes = 33 | Steps = 31852 | Steps Per Second = 201.841\n",
      "[Agent] Critic Loss = 1.976 | Policy Loss = 0.036 | Steps = 4111 | Walltime = 244.625\n",
      "[Env Loop] Episode Length = 352 | Episode Return = 81.1127700805664 | Episodes = 39 | Steps = 34092 | Steps Per Second = 211.762\n",
      "[Agent] Critic Loss = 1.919 | Policy Loss = 0.012 | Steps = 4370 | Walltime = 254.660\n",
      "[Env Loop] Episode Length = 993 | Episode Return = 46.13310241699219 | Episodes = 43 | Steps = 36375 | Steps Per Second = 205.548\n",
      "[Agent] Critic Loss = 1.824 | Policy Loss = 0.066 | Steps = 4614 | Walltime = 264.668\n",
      "[Env Loop] Episode Length = 223 | Episode Return = 86.9656982421875 | Episodes = 49 | Steps = 38330 | Steps Per Second = 178.632\n",
      "[Agent] Critic Loss = 1.743 | Policy Loss = 0.094 | Steps = 4858 | Walltime = 274.686\n",
      "[Env Loop] Episode Length = 177 | Episode Return = 88.24260711669922 | Episodes = 60 | Steps = 40334 | Steps Per Second = 205.764\n",
      "[Agent] Critic Loss = 1.704 | Policy Loss = 0.073 | Steps = 5107 | Walltime = 284.734\n",
      "[Env Loop] Episode Length = 243 | Episode Return = 84.65799713134766 | Episodes = 74 | Steps = 42498 | Steps Per Second = 193.042\n",
      "[Agent] Critic Loss = 1.675 | Policy Loss = 0.158 | Steps = 5345 | Walltime = 294.745\n",
      "[Env Loop] Episode Length = 142 | Episode Return = 90.10131072998047 | Episodes = 87 | Steps = 44512 | Steps Per Second = 188.533\n",
      "[Agent] Critic Loss = 1.697 | Policy Loss = 0.179 | Steps = 5574 | Walltime = 304.786\n",
      "[Env Loop] Episode Length = 145 | Episode Return = 90.76753234863281 | Episodes = 100 | Steps = 46421 | Steps Per Second = 203.892\n",
      "[Agent] Critic Loss = 1.750 | Policy Loss = 0.293 | Steps = 5826 | Walltime = 314.806\n",
      "[Env Loop] Episode Length = 114 | Episode Return = 91.94100952148438 | Episodes = 117 | Steps = 48431 | Steps Per Second = 167.057\n",
      "[Agent] Critic Loss = 1.770 | Policy Loss = 0.299 | Steps = 6036 | Walltime = 324.816\n",
      "[Env Loop] Episode Length = 123 | Episode Return = 91.91919708251953 | Episodes = 131 | Steps = 50182 | Steps Per Second = 189.576\n",
      "[Agent] Critic Loss = 1.821 | Policy Loss = 0.380 | Steps = 6255 | Walltime = 334.838\n",
      "[Env Loop] Episode Length = 152 | Episode Return = 90.42987060546875 | Episodes = 148 | Steps = 52137 | Steps Per Second = 200.817\n",
      "[Agent] Critic Loss = 1.859 | Policy Loss = 0.304 | Steps = 6488 | Walltime = 344.848\n",
      "[Env Loop] Episode Length = 145 | Episode Return = 89.45098876953125 | Episodes = 164 | Steps = 54030 | Steps Per Second = 185.186\n",
      "[Agent] Critic Loss = 2.033 | Policy Loss = 0.348 | Steps = 6703 | Walltime = 354.859\n",
      "[Env Loop] Episode Length = 154 | Episode Return = 89.9810562133789 | Episodes = 179 | Steps = 55862 | Steps Per Second = 189.913\n",
      "[Agent] Critic Loss = 2.045 | Policy Loss = 0.372 | Steps = 6934 | Walltime = 364.864\n",
      "[Env Loop] Episode Length = 81 | Episode Return = 93.99153137207031 | Episodes = 194 | Steps = 57761 | Steps Per Second = 198.907\n",
      "[Agent] Critic Loss = 2.042 | Policy Loss = 0.408 | Steps = 7166 | Walltime = 374.898\n",
      "[Env Loop] Episode Length = 141 | Episode Return = 90.34214782714844 | Episodes = 210 | Steps = 59757 | Steps Per Second = 195.005\n",
      "[Agent] Critic Loss = 2.060 | Policy Loss = 0.348 | Steps = 7410 | Walltime = 384.928\n",
      "[Env Loop] Episode Length = 112 | Episode Return = 92.55487060546875 | Episodes = 225 | Steps = 61673 | Steps Per Second = 169.124\n",
      "[Agent] Critic Loss = 2.026 | Policy Loss = 0.319 | Steps = 7640 | Walltime = 394.949\n",
      "[Env Loop] Episode Length = 111 | Episode Return = 92.4317626953125 | Episodes = 241 | Steps = 63621 | Steps Per Second = 195.349\n",
      "[Agent] Critic Loss = 1.986 | Policy Loss = 0.364 | Steps = 7877 | Walltime = 404.960\n",
      "[Env Loop] Episode Length = 115 | Episode Return = 92.28339385986328 | Episodes = 257 | Steps = 65622 | Steps Per Second = 201.789\n",
      "[Agent] Critic Loss = 1.957 | Policy Loss = 0.341 | Steps = 8121 | Walltime = 414.963\n",
      "[Env Loop] Episode Length = 115 | Episode Return = 92.76998138427734 | Episodes = 273 | Steps = 67501 | Steps Per Second = 171.386\n",
      "[Agent] Critic Loss = 1.934 | Policy Loss = 0.319 | Steps = 8339 | Walltime = 424.998\n",
      "[Env Loop] Episode Length = 140 | Episode Return = 90.16606903076172 | Episodes = 289 | Steps = 69415 | Steps Per Second = 197.554\n",
      "[Agent] Critic Loss = 1.927 | Policy Loss = 0.292 | Steps = 8580 | Walltime = 435.009\n",
      "[Env Loop] Episode Length = 87 | Episode Return = 93.4610366821289 | Episodes = 305 | Steps = 71330 | Steps Per Second = 184.598\n",
      "[Agent] Critic Loss = 1.899 | Policy Loss = 0.325 | Steps = 8817 | Walltime = 445.021\n",
      "[Env Loop] Episode Length = 138 | Episode Return = 91.1297378540039 | Episodes = 322 | Steps = 73278 | Steps Per Second = 181.180\n",
      "[Agent] Critic Loss = 1.919 | Policy Loss = 0.305 | Steps = 9049 | Walltime = 455.064\n",
      "[Env Loop] Episode Length = 116 | Episode Return = 92.26724243164062 | Episodes = 337 | Steps = 75042 | Steps Per Second = 160.276\n",
      "[Agent] Critic Loss = 1.862 | Policy Loss = 0.293 | Steps = 9271 | Walltime = 465.111\n",
      "[Env Loop] Episode Length = 106 | Episode Return = 92.23666381835938 | Episodes = 353 | Steps = 76898 | Steps Per Second = 155.973\n",
      "[Agent] Critic Loss = 1.888 | Policy Loss = 0.308 | Steps = 9491 | Walltime = 475.150\n",
      "[Agent] Critic Loss = 1.889 | Policy Loss = 0.318 | Steps = 9706 | Walltime = 485.184\n",
      "[Env Loop] Episode Length = 122 | Episode Return = 92.09983825683594 | Episodes = 368 | Steps = 78656 | Steps Per Second = 142.532\n",
      "[Agent] Critic Loss = 1.872 | Policy Loss = 0.269 | Steps = 9911 | Walltime = 495.199\n",
      "[Env Loop] Episode Length = 104 | Episode Return = 93.69390869140625 | Episodes = 383 | Steps = 80362 | Steps Per Second = 169.242\n",
      "[Agent] Critic Loss = 1.883 | Policy Loss = 0.340 | Steps = 10117 | Walltime = 505.216\n",
      "[Env Loop] Episode Length = 109 | Episode Return = 93.09486389160156 | Episodes = 397 | Steps = 82085 | Steps Per Second = 195.255\n",
      "[Agent] Critic Loss = 1.909 | Policy Loss = 0.272 | Steps = 10345 | Walltime = 515.232\n",
      "[Env Loop] Episode Length = 109 | Episode Return = 93.22736358642578 | Episodes = 414 | Steps = 83977 | Steps Per Second = 183.477\n",
      "[Agent] Critic Loss = 1.841 | Policy Loss = 0.303 | Steps = 10581 | Walltime = 525.246\n",
      "[Env Loop] Episode Length = 111 | Episode Return = 92.7179183959961 | Episodes = 430 | Steps = 85900 | Steps Per Second = 193.148\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Agent] Critic Loss = 1.937 | Policy Loss = 0.276 | Steps = 10821 | Walltime = 535.261\n",
      "[Env Loop] Episode Length = 174 | Episode Return = 88.50698852539062 | Episodes = 446 | Steps = 87978 | Steps Per Second = 189.055\n",
      "[Agent] Critic Loss = 1.841 | Policy Loss = 0.275 | Steps = 11062 | Walltime = 545.292\n",
      "[Env Loop] Episode Length = 167 | Episode Return = 88.38923645019531 | Episodes = 462 | Steps = 89950 | Steps Per Second = 193.673\n",
      "[Agent] Critic Loss = 1.855 | Policy Loss = 0.276 | Steps = 11297 | Walltime = 555.298\n",
      "[Env Loop] Episode Length = 110 | Episode Return = 92.47654724121094 | Episodes = 477 | Steps = 91824 | Steps Per Second = 186.733\n",
      "[Agent] Critic Loss = 1.873 | Policy Loss = 0.333 | Steps = 11529 | Walltime = 565.298\n",
      "[Env Loop] Episode Length = 112 | Episode Return = 91.98640441894531 | Episodes = 494 | Steps = 93765 | Steps Per Second = 188.566\n",
      "[Agent] Critic Loss = 1.825 | Policy Loss = 0.290 | Steps = 11769 | Walltime = 575.327\n",
      "[Env Loop] Episode Length = 139 | Episode Return = 91.43666076660156 | Episodes = 510 | Steps = 95706 | Steps Per Second = 190.156\n",
      "[Agent] Critic Loss = 1.730 | Policy Loss = 0.276 | Steps = 12008 | Walltime = 585.329\n",
      "[Env Loop] Episode Length = 111 | Episode Return = 92.74910736083984 | Episodes = 526 | Steps = 97680 | Steps Per Second = 183.364\n",
      "[Agent] Critic Loss = 1.821 | Policy Loss = 0.308 | Steps = 12250 | Walltime = 595.332\n",
      "[Env Loop] Episode Length = 104 | Episode Return = 92.79008483886719 | Episodes = 544 | Steps = 99665 | Steps Per Second = 200.228\n",
      "[Agent] Critic Loss = 1.848 | Policy Loss = 0.318 | Steps = 12490 | Walltime = 605.364\n",
      "[Env Loop] Episode Length = 97 | Episode Return = 93.1600570678711 | Episodes = 562 | Steps = 101571 | Steps Per Second = 183.779\n",
      "[Agent] Critic Loss = 1.831 | Policy Loss = 0.323 | Steps = 12721 | Walltime = 615.384\n",
      "[Env Loop] Episode Length = 110 | Episode Return = 92.03665924072266 | Episodes = 578 | Steps = 103484 | Steps Per Second = 190.156\n",
      "[Agent] Critic Loss = 1.834 | Policy Loss = 0.285 | Steps = 12961 | Walltime = 625.403\n",
      "[Env Loop] Episode Length = 107 | Episode Return = 92.47149658203125 | Episodes = 595 | Steps = 105483 | Steps Per Second = 194.394\n",
      "[Agent] Critic Loss = 1.794 | Policy Loss = 0.269 | Steps = 13200 | Walltime = 635.437\n",
      "[Env Loop] Episode Length = 104 | Episode Return = 92.60198974609375 | Episodes = 612 | Steps = 107420 | Steps Per Second = 190.113\n",
      "[Agent] Critic Loss = 1.787 | Policy Loss = 0.281 | Steps = 13435 | Walltime = 645.465\n",
      "[Env Loop] Episode Length = 97 | Episode Return = 93.67766571044922 | Episodes = 629 | Steps = 109300 | Steps Per Second = 189.511\n",
      "[Agent] Critic Loss = 1.789 | Policy Loss = 0.304 | Steps = 13650 | Walltime = 655.534\n",
      "[Env Loop] Episode Length = 143 | Episode Return = 90.74729919433594 | Episodes = 644 | Steps = 111063 | Steps Per Second = 185.604\n",
      "[Agent] Critic Loss = 1.744 | Policy Loss = 0.270 | Steps = 13861 | Walltime = 665.599\n",
      "[Env Loop] Episode Length = 110 | Episode Return = 92.83794403076172 | Episodes = 659 | Steps = 112811 | Steps Per Second = 183.162\n",
      "[Agent] Critic Loss = 1.802 | Policy Loss = 0.265 | Steps = 14076 | Walltime = 675.626\n",
      "[Env Loop] Episode Length = 107 | Episode Return = 92.95440673828125 | Episodes = 674 | Steps = 114630 | Steps Per Second = 189.582\n",
      "[Agent] Critic Loss = 1.770 | Policy Loss = 0.262 | Steps = 14303 | Walltime = 685.635\n",
      "[Env Loop] Episode Length = 108 | Episode Return = 92.66163635253906 | Episodes = 690 | Steps = 116581 | Steps Per Second = 196.351\n",
      "[Agent] Critic Loss = 1.787 | Policy Loss = 0.312 | Steps = 14539 | Walltime = 695.660\n",
      "[Env Loop] Episode Length = 109 | Episode Return = 92.81655883789062 | Episodes = 707 | Steps = 118556 | Steps Per Second = 191.940\n",
      "[Agent] Critic Loss = 1.789 | Policy Loss = 0.266 | Steps = 14778 | Walltime = 705.668\n",
      "[Env Loop] Episode Length = 105 | Episode Return = 92.76535034179688 | Episodes = 724 | Steps = 120546 | Steps Per Second = 189.693\n",
      "[Agent] Critic Loss = 1.715 | Policy Loss = 0.178 | Steps = 15016 | Walltime = 715.682\n",
      "[Env Loop] Episode Length = 111 | Episode Return = 93.07837677001953 | Episodes = 741 | Steps = 122480 | Steps Per Second = 191.958\n",
      "[Agent] Critic Loss = 1.778 | Policy Loss = 0.312 | Steps = 15253 | Walltime = 725.685\n",
      "[Env Loop] Episode Length = 122 | Episode Return = 91.96144104003906 | Episodes = 758 | Steps = 124453 | Steps Per Second = 195.107\n",
      "[Agent] Critic Loss = 1.702 | Policy Loss = 0.218 | Steps = 15489 | Walltime = 735.697\n",
      "[Env Loop] Episode Length = 110 | Episode Return = 92.29054260253906 | Episodes = 775 | Steps = 126322 | Steps Per Second = 190.811\n",
      "[Agent] Critic Loss = 1.749 | Policy Loss = 0.244 | Steps = 15723 | Walltime = 745.708\n",
      "[Env Loop] Episode Length = 122 | Episode Return = 91.9982681274414 | Episodes = 792 | Steps = 128282 | Steps Per Second = 186.313\n",
      "[Agent] Critic Loss = 1.783 | Policy Loss = 0.274 | Steps = 15965 | Walltime = 755.738\n",
      "[Env Loop] Episode Length = 105 | Episode Return = 92.92359161376953 | Episodes = 808 | Steps = 130279 | Steps Per Second = 189.469\n",
      "[Agent] Critic Loss = 1.771 | Policy Loss = 0.230 | Steps = 16205 | Walltime = 765.742\n",
      "[Env Loop] Episode Length = 110 | Episode Return = 92.68746948242188 | Episodes = 826 | Steps = 132303 | Steps Per Second = 179.489\n",
      "[Agent] Critic Loss = 1.799 | Policy Loss = 0.260 | Steps = 16447 | Walltime = 775.770\n",
      "[Env Loop] Episode Length = 192 | Episode Return = 86.8713150024414 | Episodes = 843 | Steps = 134269 | Steps Per Second = 192.798\n",
      "[Agent] Critic Loss = 1.861 | Policy Loss = 0.321 | Steps = 16690 | Walltime = 785.804\n",
      "[Env Loop] Episode Length = 111 | Episode Return = 92.55802917480469 | Episodes = 861 | Steps = 136207 | Steps Per Second = 206.935\n",
      "[Agent] Critic Loss = 1.737 | Policy Loss = 0.243 | Steps = 16933 | Walltime = 795.837\n",
      "[Env Loop] Episode Length = 110 | Episode Return = 92.92042541503906 | Episodes = 878 | Steps = 138216 | Steps Per Second = 193.537\n",
      "[Agent] Critic Loss = 1.813 | Policy Loss = 0.252 | Steps = 17181 | Walltime = 805.883\n",
      "[Env Loop] Episode Length = 112 | Episode Return = 92.53562927246094 | Episodes = 895 | Steps = 140233 | Steps Per Second = 190.220\n",
      "[Agent] Critic Loss = 1.768 | Policy Loss = 0.244 | Steps = 17423 | Walltime = 815.912\n",
      "[Env Loop] Episode Length = 108 | Episode Return = 92.59315490722656 | Episodes = 912 | Steps = 142193 | Steps Per Second = 188.303\n",
      "[Agent] Critic Loss = 1.748 | Policy Loss = 0.228 | Steps = 17665 | Walltime = 825.946\n",
      "[Env Loop] Episode Length = 103 | Episode Return = 92.14220428466797 | Episodes = 929 | Steps = 144166 | Steps Per Second = 187.344\n",
      "[Agent] Critic Loss = 1.767 | Policy Loss = 0.277 | Steps = 17904 | Walltime = 835.958\n",
      "[Agent] Critic Loss = 1.759 | Policy Loss = 0.269 | Steps = 18138 | Walltime = 845.974\n",
      "[Env Loop] Episode Length = 170 | Episode Return = 88.68511962890625 | Episodes = 946 | Steps = 146113 | Steps Per Second = 171.263\n",
      "[Agent] Critic Loss = 1.756 | Policy Loss = 0.260 | Steps = 18376 | Walltime = 856.011\n",
      "[Env Loop] Episode Length = 113 | Episode Return = 92.25672912597656 | Episodes = 963 | Steps = 148065 | Steps Per Second = 195.200\n",
      "[Agent] Critic Loss = 1.683 | Policy Loss = 0.255 | Steps = 18604 | Walltime = 866.052\n",
      "[Env Loop] Episode Length = 102 | Episode Return = 93.19791412353516 | Episodes = 979 | Steps = 149962 | Steps Per Second = 188.334\n",
      "[Agent] Critic Loss = 1.687 | Policy Loss = 0.213 | Steps = 18836 | Walltime = 876.082\n",
      "[Env Loop] Episode Length = 73 | Episode Return = 94.51622009277344 | Episodes = 994 | Steps = 151854 | Steps Per Second = 183.835\n",
      "[Agent] Critic Loss = 1.714 | Policy Loss = 0.245 | Steps = 19068 | Walltime = 886.088\n",
      "[Env Loop] Episode Length = 106 | Episode Return = 92.64910125732422 | Episodes = 1010 | Steps = 153737 | Steps Per Second = 182.089\n"
     ]
    }
   ],
   "source": [
    "env_loop.run(num_episodes=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "widespread-channels",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-3a95e0057d57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0menvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0menvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.0125\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "timestep = environment.reset()\n",
    "action = agent.select_action(timestep.observation)\n",
    "\n",
    "while not timestep.last():\n",
    "    environment.render()\n",
    "    environment.step(action)\n",
    "    time.sleep(0.0125)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
